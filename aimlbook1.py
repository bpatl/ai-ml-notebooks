# -*- coding: utf-8 -*-
"""aimlbook1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14-3yKFQ1bA6Y0FUn5uq_vAZtGtv6OAR0

Step 1: Import Tensorflow library
"""

import tensorflow as tf

"""Step 2 : Check that you have GPU(s) available on your notebook"""

tf.test.gpu_device_name()

# Get the list of all logical GPU device on your notebook
GPU_DEVICES = tf.config.list_logical_devices('GPU')
# Get the list of all logical CPU device on your notebook
CPU_DEVICES = tf.config.list_logical_devices('CPU')
# Keep only the names of each GPU devices
GPU_DEVICES_NAMES = [x.name for x in GPU_DEVICES]
# Keep only the names of each CPU devices
CPU_DEVICES_NAMES = [x.name for x in CPU_DEVICES]
# The number of GPU devices
GPU_DEVICES_NB = len(GPU_DEVICES)
# The number of CPU devices
CPU_DEVICES_NB = len(CPU_DEVICES)

if GPU_DEVICES_NB == 0:
    raise SystemError('No GPU device found')
    print(f'{GPU_DEVICES_NB} No GPU device found have been found on your notebook :')
else:
    print(f'{GPU_DEVICES_NB} GPU device(s) have been found on your notebook :')

for nb in range(GPU_DEVICES_NB):
    gpu_name = GPU_DEVICES_NAMES[nb]
    print(f'* GPU n°{nb} whose name is "{gpu_name}"')

print('')

if CPU_DEVICES_NB == 0:
    raise SystemError('No CPU device found')
else:
    print(f'{CPU_DEVICES_NB} CPU device(s) have been found on your notebook :')

for nb in range(CPU_DEVICES_NB):
    cpu_name = CPU_DEVICES_NAMES[nb]
    print(f'* CPU n°{nb} whose name is "{cpu_name}"')

"""Step 3 : Define the operation to benchmark"""

def random_multiply(vector_length):
    vector_1 = tf.random.normal(vector_length)
    vector_2 = tf.random.normal(vector_length)
    return vector_1 * vector_2

"""Step 4 : Define the function executing the operation on GPU device"""

def gpu_operation(vector_length):
    # If you have several GPU you can select the one to use by changing the used index of GPU_DEVICES_NAMES
    with tf.device(GPU_DEVICES_NAMES[0]):
        random_multiply(vector_length)

"""Step 5 : Define the function executing the operation on CPU device"""

def cpu_operation(vector_length):
    # If you have several CPU you can select the one to use by changing the used index of GPU_DEVICES_NAMES
    with tf.device(CPU_DEVICES_NAMES[0]):
        random_multiply(vector_length)

"""Step6 : Launch the benchmark of each device over several vectors of different lengths

Here we are going to iterate over several lengths of vectors and launch a benchmark both on GPU and CPU to observe on which cases GPU is better.
"""

import timeit

# We run each op once to warm up; see: https://stackoverflow.com/a/45067900
cpu_operation([1])
gpu_operation([1])

for i in range(8):
    vector_length = pow(10, i)
    cpu_time = timeit.timeit(f'cpu_operation([{vector_length}])', number=20, setup="from __main__ import cpu_operation")
    gpu_time = timeit.timeit(f'gpu_operation([{vector_length}])', number=20, setup="from __main__ import gpu_operation")
    print(f'Operations on vector of length {vector_length} are {cpu_time/gpu_time}x faster on GPU than CPU')

"""Going further"""

import os

NOTEBOOK_ID = os.environ.get('NOTEBOOK_ID')
JOB_ID = os.environ.get('JOB_ID')
NOTEBOOK_HOST = os.environ.get('NOTEBOOK_HOST')
JOB_HOST = os.environ.get('JOB_HOST')
print(f'NOTEBOOK_ID {NOTEBOOK_ID} ')
print(f'JOB_ID {JOB_ID} ')
print(f'NOTEBOOK_HOST {NOTEBOOK_HOST} ')
print(f'JOB_HOST {JOB_HOST} ')

if NOTEBOOK_ID and NOTEBOOK_HOST:
    VARID = "var-notebook=" + NOTEBOOK_ID
    HOST = NOTEBOOK_HOST
    SUBDOMAIN = "notebook"
elif JOB_ID and JOB_HOST:
    VARID = "var-job=" + JOB_ID
    HOST = JOB_HOST
    SUBDOMAIN = "job"

print(f'Your resource monitoring dashboard URL is:')

import os

# Print specific key environment variables
print("HOME:", os.environ.get('HOME'))
print("USER:", os.environ.get('USER'))
print("PATH:", os.environ.get('PATH'))
print("SHELL:", os.environ.get('SHELL'))

# Optionally, print all environment variables:
print("\nAll environment variables:\n")
for key, value in os.environ.items():
    print(f'{key}: {value}')