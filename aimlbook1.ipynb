{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Import Tensorflow library"
      ],
      "metadata": {
        "id": "lZ-e7EgqFop9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "ZKbT6120Fp5c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 : Check that you have GPU(s) available on your notebook"
      ],
      "metadata": {
        "id": "5lisUJinFx0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sw5M0LIOF5e8",
        "outputId": "e0dd6e33-95e9-4329-bb27-20360629ddbd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of all logical GPU device on your notebook\n",
        "GPU_DEVICES = tf.config.list_logical_devices('GPU')\n",
        "# Get the list of all logical CPU device on your notebook\n",
        "CPU_DEVICES = tf.config.list_logical_devices('CPU')\n",
        "# Keep only the names of each GPU devices\n",
        "GPU_DEVICES_NAMES = [x.name for x in GPU_DEVICES]\n",
        "# Keep only the names of each CPU devices\n",
        "CPU_DEVICES_NAMES = [x.name for x in CPU_DEVICES]\n",
        "# The number of GPU devices\n",
        "GPU_DEVICES_NB = len(GPU_DEVICES)\n",
        "# The number of CPU devices\n",
        "CPU_DEVICES_NB = len(CPU_DEVICES)\n",
        "\n",
        "if GPU_DEVICES_NB == 0:\n",
        "    raise SystemError('No GPU device found')\n",
        "    print(f'{GPU_DEVICES_NB} No GPU device found have been found on your notebook :')\n",
        "else:\n",
        "    print(f'{GPU_DEVICES_NB} GPU device(s) have been found on your notebook :')\n",
        "\n",
        "for nb in range(GPU_DEVICES_NB):\n",
        "    gpu_name = GPU_DEVICES_NAMES[nb]\n",
        "    print(f'* GPU n째{nb} whose name is \"{gpu_name}\"')\n",
        "\n",
        "print('')\n",
        "\n",
        "if CPU_DEVICES_NB == 0:\n",
        "    raise SystemError('No CPU device found')\n",
        "else:\n",
        "    print(f'{CPU_DEVICES_NB} CPU device(s) have been found on your notebook :')\n",
        "\n",
        "for nb in range(CPU_DEVICES_NB):\n",
        "    cpu_name = CPU_DEVICES_NAMES[nb]\n",
        "    print(f'* CPU n째{nb} whose name is \"{cpu_name}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBsyYz4bG67G",
        "outputId": "0c263263-e192-4284-ea6f-6189110a75dc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 GPU device(s) have been found on your notebook :\n",
            "* GPU n째0 whose name is \"/device:GPU:0\"\n",
            "\n",
            "1 CPU device(s) have been found on your notebook :\n",
            "* CPU n째0 whose name is \"/device:CPU:0\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 : Define the operation to benchmark"
      ],
      "metadata": {
        "id": "GRS4btk7LjhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_multiply(vector_length):\n",
        "    vector_1 = tf.random.normal(vector_length)\n",
        "    vector_2 = tf.random.normal(vector_length)\n",
        "    return vector_1 * vector_2"
      ],
      "metadata": {
        "id": "2dYqMoMjLk5a"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 : Define the function executing the operation on GPU device"
      ],
      "metadata": {
        "id": "8E_lelPSLqZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gpu_operation(vector_length):\n",
        "    # If you have several GPU you can select the one to use by changing the used index of GPU_DEVICES_NAMES\n",
        "    with tf.device(GPU_DEVICES_NAMES[0]):\n",
        "        random_multiply(vector_length)"
      ],
      "metadata": {
        "id": "mCdSw8LILruM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5 : Define the function executing the operation on CPU device"
      ],
      "metadata": {
        "id": "nxJtttRtLuMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cpu_operation(vector_length):\n",
        "    # If you have several CPU you can select the one to use by changing the used index of GPU_DEVICES_NAMES\n",
        "    with tf.device(CPU_DEVICES_NAMES[0]):\n",
        "        random_multiply(vector_length)"
      ],
      "metadata": {
        "id": "mUci6Qi7Lvx5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step6 : Launch the benchmark of each device over several vectors of different lengths"
      ],
      "metadata": {
        "id": "Ovlm_eVML0py"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to iterate over several lengths of vectors and launch a benchmark both on GPU and CPU to observe on which cases GPU is better."
      ],
      "metadata": {
        "id": "CZIA7oGXL470"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "\n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu_operation([1])\n",
        "gpu_operation([1])\n",
        "\n",
        "for i in range(8):\n",
        "    vector_length = pow(10, i)\n",
        "    cpu_time = timeit.timeit(f'cpu_operation([{vector_length}])', number=20, setup=\"from __main__ import cpu_operation\")\n",
        "    gpu_time = timeit.timeit(f'gpu_operation([{vector_length}])', number=20, setup=\"from __main__ import gpu_operation\")\n",
        "    print(f'Operations on vector of length {vector_length} are {cpu_time/gpu_time}x faster on GPU than CPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQE-TG1mL5rg",
        "outputId": "ce00703b-38aa-4701-973f-e295faa2bfed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Operations on vector of length 1 are 0.6987619177561214x faster on GPU than CPU\n",
            "Operations on vector of length 10 are 0.7109028601921361x faster on GPU than CPU\n",
            "Operations on vector of length 100 are 0.6604102426570363x faster on GPU than CPU\n",
            "Operations on vector of length 1000 are 0.8609441065413451x faster on GPU than CPU\n",
            "Operations on vector of length 10000 are 1.7294900869575436x faster on GPU than CPU\n",
            "Operations on vector of length 100000 are 5.83715190042702x faster on GPU than CPU\n",
            "Operations on vector of length 1000000 are 40.03473374728364x faster on GPU than CPU\n",
            "Operations on vector of length 10000000 are 480.4402382019885x faster on GPU than CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Going further"
      ],
      "metadata": {
        "id": "wxbR6acUME06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "NOTEBOOK_ID = os.environ.get('NOTEBOOK_ID')\n",
        "JOB_ID = os.environ.get('JOB_ID')\n",
        "NOTEBOOK_HOST = os.environ.get('NOTEBOOK_HOST')\n",
        "JOB_HOST = os.environ.get('JOB_HOST')\n",
        "print(f'NOTEBOOK_ID {NOTEBOOK_ID} ')\n",
        "print(f'JOB_ID {JOB_ID} ')\n",
        "print(f'NOTEBOOK_HOST {NOTEBOOK_HOST} ')\n",
        "print(f'JOB_HOST {JOB_HOST} ')\n",
        "\n",
        "if NOTEBOOK_ID and NOTEBOOK_HOST:\n",
        "    VARID = \"var-notebook=\" + NOTEBOOK_ID\n",
        "    HOST = NOTEBOOK_HOST\n",
        "    SUBDOMAIN = \"notebook\"\n",
        "elif JOB_ID and JOB_HOST:\n",
        "    VARID = \"var-job=\" + JOB_ID\n",
        "    HOST = JOB_HOST\n",
        "    SUBDOMAIN = \"job\"\n",
        "\n",
        "print(f'Your resource monitoring dashboard URL is:')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1VKGbtuMFjw",
        "outputId": "23d029db-1d93-4376-ebcd-40f4db8b1bdd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NOTEBOOK_ID None \n",
            "JOB_ID None \n",
            "NOTEBOOK_HOST None \n",
            "JOB_HOST None \n",
            "Your resource monitoring dashboard URL is:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Print specific key environment variables\n",
        "print(\"HOME:\", os.environ.get('HOME'))\n",
        "print(\"USER:\", os.environ.get('USER'))\n",
        "print(\"PATH:\", os.environ.get('PATH'))\n",
        "print(\"SHELL:\", os.environ.get('SHELL'))\n",
        "\n",
        "# Optionally, print all environment variables:\n",
        "print(\"\\nAll environment variables:\\n\")\n",
        "for key, value in os.environ.items():\n",
        "    print(f'{key}: {value}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1vlENQWNXyW",
        "outputId": "b2f2c535-ec12-4772-98a7-8fd8eaac53c1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HOME: /root\n",
            "USER: None\n",
            "PATH: /opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n",
            "SHELL: /bin/bash\n",
            "\n",
            "All environment variables:\n",
            "\n",
            "SHELL: /bin/bash\n",
            "NV_LIBCUBLAS_VERSION: 12.2.5.6-1\n",
            "NVIDIA_VISIBLE_DEVICES: all\n",
            "COLAB_JUPYTER_TRANSPORT: ipc\n",
            "NV_NVML_DEV_VERSION: 12.2.140-1\n",
            "NV_CUDNN_PACKAGE_NAME: libcudnn8\n",
            "CGROUP_MEMORY_EVENTS: /sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events\n",
            "NV_LIBNCCL_DEV_PACKAGE: libnccl-dev=2.19.3-1+cuda12.2\n",
            "NV_LIBNCCL_DEV_PACKAGE_VERSION: 2.19.3-1\n",
            "VM_GCE_METADATA_HOST: 169.254.169.253\n",
            "HOSTNAME: 482594a38985\n",
            "LANGUAGE: en_US\n",
            "TBE_RUNTIME_ADDR: 172.28.0.1:8011\n",
            "COLAB_TPU_1VM: \n",
            "GCE_METADATA_TIMEOUT: 3\n",
            "NVIDIA_REQUIRE_CUDA: cuda>=12.2 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\n",
            "NV_LIBCUBLAS_DEV_PACKAGE: libcublas-dev-12-2=12.2.5.6-1\n",
            "NV_NVTX_VERSION: 12.2.140-1\n",
            "COLAB_JUPYTER_IP: 172.28.0.12\n",
            "NV_CUDA_CUDART_DEV_VERSION: 12.2.140-1\n",
            "NV_LIBCUSPARSE_VERSION: 12.1.2.141-1\n",
            "COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL: http://172.28.0.1:8013/\n",
            "NV_LIBNPP_VERSION: 12.2.1.4-1\n",
            "NCCL_VERSION: 2.19.3-1\n",
            "KMP_LISTEN_PORT: 6000\n",
            "TF_FORCE_GPU_ALLOW_GROWTH: true\n",
            "ENV: /root/.bashrc\n",
            "PWD: /\n",
            "TBE_EPHEM_CREDS_ADDR: 172.28.0.1:8009\n",
            "COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT: 30s\n",
            "TBE_CREDS_ADDR: 172.28.0.1:8008\n",
            "NV_CUDNN_PACKAGE: libcudnn8=8.9.6.50-1+cuda12.2\n",
            "NVIDIA_DRIVER_CAPABILITIES: compute,utility\n",
            "COLAB_JUPYTER_TOKEN: \n",
            "LAST_FORCED_REBUILD: 20241007\n",
            "NV_NVPROF_DEV_PACKAGE: cuda-nvprof-12-2=12.2.142-1\n",
            "NV_LIBNPP_PACKAGE: libnpp-12-2=12.2.1.4-1\n",
            "NV_LIBNCCL_DEV_PACKAGE_NAME: libnccl-dev\n",
            "TCLLIBPATH: /usr/share/tcltk/tcllib1.20\n",
            "NV_LIBCUBLAS_DEV_VERSION: 12.2.5.6-1\n",
            "NVIDIA_PRODUCT_NAME: CUDA\n",
            "COLAB_KERNEL_MANAGER_PROXY_HOST: 172.28.0.12\n",
            "NV_LIBCUBLAS_DEV_PACKAGE_NAME: libcublas-dev-12-2\n",
            "NV_CUDA_CUDART_VERSION: 12.2.140-1\n",
            "COLAB_WARMUP_DEFAULTS: 1\n",
            "HOME: /root\n",
            "LANG: en_US.UTF-8\n",
            "COLUMNS: 100\n",
            "CUDA_VERSION: 12.2.2\n",
            "CLOUDSDK_CONFIG: /content/.config\n",
            "NV_LIBCUBLAS_PACKAGE: libcublas-12-2=12.2.5.6-1\n",
            "NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE: cuda-nsight-compute-12-2=12.2.2-1\n",
            "COLAB_RELEASE_TAG: release-colab_20241010-060117_RC00\n",
            "KMP_TARGET_PORT: 9000\n",
            "KMP_EXTRA_ARGS: --logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-hubp745vconr --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true\n",
            "NV_LIBNPP_DEV_PACKAGE: libnpp-dev-12-2=12.2.1.4-1\n",
            "COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS: /datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages\n",
            "NV_LIBCUBLAS_PACKAGE_NAME: libcublas-12-2\n",
            "COLAB_KERNEL_MANAGER_PROXY_PORT: 6000\n",
            "CLOUDSDK_PYTHON: python3\n",
            "NV_LIBNPP_DEV_VERSION: 12.2.1.4-1\n",
            "NO_GCE_CHECK: False\n",
            "PYTHONPATH: /env/python\n",
            "NV_LIBCUSPARSE_DEV_VERSION: 12.1.2.141-1\n",
            "LIBRARY_PATH: /usr/local/cuda/lib64/stubs\n",
            "NV_CUDNN_VERSION: 8.9.6.50\n",
            "SHLVL: 0\n",
            "NV_CUDA_LIB_VERSION: 12.2.2-1\n",
            "COLAB_LANGUAGE_SERVER_PROXY: /usr/colab/bin/language_service\n",
            "NVARCH: x86_64\n",
            "NV_CUDNN_PACKAGE_DEV: libcudnn8-dev=8.9.6.50-1+cuda12.2\n",
            "NV_CUDA_COMPAT_PACKAGE: cuda-compat-12-2\n",
            "NV_LIBNCCL_PACKAGE: libnccl2=2.19.3-1+cuda12.2\n",
            "LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "COLAB_GPU: 1\n",
            "NV_CUDA_NSIGHT_COMPUTE_VERSION: 12.2.2-1\n",
            "GCS_READ_CACHE_BLOCK_SIZE_MB: 16\n",
            "NV_NVPROF_VERSION: 12.2.142-1\n",
            "LC_ALL: en_US.UTF-8\n",
            "COLAB_FILE_HANDLER_ADDR: localhost:3453\n",
            "PATH: /opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n",
            "NV_LIBNCCL_PACKAGE_NAME: libnccl2\n",
            "COLAB_DEBUG_ADAPTER_MUX_PATH: /usr/local/bin/dap_multiplexer\n",
            "NV_LIBNCCL_PACKAGE_VERSION: 2.19.3-1\n",
            "PYTHONWARNINGS: ignore:::pip._internal.cli.base_command\n",
            "DEBIAN_FRONTEND: noninteractive\n",
            "COLAB_BACKEND_VERSION: next\n",
            "OLDPWD: /\n",
            "JPY_PARENT_PID: 117\n",
            "TERM: xterm-color\n",
            "CLICOLOR: 1\n",
            "PAGER: cat\n",
            "GIT_PAGER: cat\n",
            "MPLBACKEND: module://ipykernel.pylab.backend_inline\n",
            "ENABLE_DIRECTORYPREFETCHER: 1\n",
            "USE_AUTH_EPHEM: 1\n",
            "PYDEVD_USE_FRAME_EVAL: NO\n",
            "TF2_BEHAVIOR: 1\n",
            "TPU_ML_PLATFORM: Tensorflow\n",
            "TPU_ML_PLATFORM_VERSION: 2.17.0\n",
            "TF_CPP_MIN_LOG_LEVEL: 1\n"
          ]
        }
      ]
    }
  ]
}